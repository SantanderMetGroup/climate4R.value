{
    "collab_server" : "",
    "contents" : "#     biasCorrection.R Bias correction methods\n#\n#     Copyright (C) 2017 Santander Meteorology Group (http://www.meteo.unican.es)\n#\n#     This program is free software: you can redistribute it and/or modify\n#     it under the terms of the GNU General Public License as published by\n#     the Free Software Foundation, either version 3 of the License, or\n#     (at your option) any later version.\n# \n#     This program is distributed in the hope that it will be useful,\n#     but WITHOUT ANY WARRANTY; without even the implied warranty of\n#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#     GNU General Public License for more details.\n# \n#     You should have received a copy of the GNU General Public License\n#     along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n#' @title Bias correction methods\n#' @description Implementation of several standard bias correction methods\n#'\n#' @template templateObsPredSim\n#' @param method method applied. Current accepted values are \\code{\"eqm\"}, \\code{\"delta\"},\n#'  \\code{\"scaling\"}, \\code{\"pqm\"} and \\code{\"gpqm\"} \\code{\"variance\"},\\code{\"loci\"} and \\code{\"ptr\"}. See details.\n#' @param precipitation Logical for precipitation data (default to FALSE). If TRUE Adjusts precipitation \n#' frequency in 'x' (prediction) to the observed frequency in 'y'. This is a preprocess to bias correct \n#' precipitation data following ThemeÃŸl et al. (2012). To adjust the frequency, \n#' parameter \\code{wet.threshold} is used (see below).\n#' @param cross.val Logical (default to FALSE). Should cross-validation be performed? methods available are \n#' leave-one-out (\"loo\") and k-fold (\"kfold\") on an annual basis. The default option (\"none\") does not \n#' perform cross-validation.\n#' @param folds Only requiered if \\code{cross.val = \"kfold\"}. A list of vectors, each containing the years \n#' to be grouped in the corresponding fold.\n#' @param wet.threshold The minimum value that is considered as a non-zero precipitation. Ignored when \n#' \\code{precipitation = FALSE}. Default to 1 (assuming mm). See details on bias correction for precipitation.\n#' @param window vector of length = 2 (or 1) specifying the time window width used to calibrate and the \n#' target days (days that are being corrected). The window is centered on the target day/s \n#' (window width >= target days). Default to \\code{NULL}, which considers the whole period.\n#' @param scaling.type Character indicating the type of the scaling method. Options are \\code{\"additive\"} (default)\n#' or \\code{\"multiplicative\"} (see details). This argument is ignored if \\code{\"scaling\"} is not \n#' selected as the bias correction method.\n#' @param  fitdistr.args Further arguments passed to function \\code{\\link[MASS]{fitdistr}} \n#' (\\code{densfun}, \\code{start}, \\code{...}). Only used when applying the \"pqm\" method \n#' (parametric quantile mapping). Please, read the \\code{\\link[MASS]{fitdistr}} help \n#' document  carefully before setting the parameters in \\code{fitdistr.args}.\n#' @param n.quantiles Integer indicating the number of quantiles to be considered when method = \"eqm\". Default is NULL, \n#' that considers all quantiles, i.e. \\code{n.quantiles = length(x[i,j])} (being \\code{i} and \\code{j} the \n#' coordinates in a single location).\n#' @param extrapolation Character indicating the extrapolation method to be applied to correct values in  \n#' \\code{newdata} that are out of the range of \\code{x}. Extrapolation is applied only to the \\code{\"eqm\"} method, \n#' thus, this argument is ignored if other bias correction method is selected. Default is \\code{\"none\"} (do not extrapolate).\n#' @param theta numeric indicating  upper threshold (and lower for the left tail of the distributions, if needed) \n#' above which precipitation (temperature) values are fitted to a Generalized Pareto Distribution (GPD). \n#' Values below this threshold are fitted to a gamma (normal) distribution. By default, 'theta' is the 95th \n#' percentile (5th percentile for the left tail). Only for \\code{\"gpqm\"} method.\n#' @param join.members Logical indicating whether members should be corrected independently (\\code{FALSE}, the default),\n#'  or joined before performing the correction (\\code{TRUE}). It applies to multimember grids only (otherwise ignored).\n#' @template templateParallelParams\n#'  \n#' @details\n#' \n#' The methods available are \\code{\"eqm\"}, \\code{\"delta\"}, \n#' \\code{\"scaling\"}, \\code{\"pqm\"}, \\code{\"gpqm\"}\\code{\"loci\"}, \n#' \\code{\"ptr\"}  (the four latter used only for precipitation) and \n#' \\code{\"variance\"} (only for temperature).\n#' \n#'  These are next briefly described: \n#'  \n#' \\strong{Delta}\n#'\n#' This method consists on adding to the observations the mean change signal (delta method).\n#' This method is applicable to any kind of variable but it is preferable to avoid it for bounded variables\n#' (e.g. precipitation, wind speed, etc.) because values out of the variable range could be obtained\n#' (e.g. negative wind speeds...). This method corresponds to case g=1 and f=0 in Amengual et al. 2012. \n#' \n#' \\strong{Scaling}\n#' \n#' This method consists on scaling the simulation  with the difference (additive) or quotient (multiplicative) \n#' between the observed and simulated means in the train period. The \\code{additive} or \\code{multiplicative}\n#' correction is defined by parameter \\code{scaling.type} (default is \\code{additive}).\n#' The additive version is preferably applicable to unbounded variables (e.g. temperature) \n#' and the multiplicative to variables with a lower bound (e.g. precipitation, because it also preserves the frequency). \n#' \n#' \n#' \\strong{eqm}\n#' \n#' Empirical Quantile Mapping. This is a very extended bias correction method which consists on calibrating the simulated Cumulative Distribution Function (CDF) \n#' by adding to the observed quantiles both the mean delta change and the individual delta changes in the corresponding quantiles. \n#' This is equivalent to f=g=1 in Amengual et al. 2012. This method is applicable to any kind of variable.\n#' \n#' \n#' \\strong{pqm}\n#' \n#' Parametric Quantile Mapping. It is based on the initial assumption that both observed and simulated intensity distributions are well approximated by a given distribution\n#' (see \\code{\\link[MASS]{fitdistr}} to check available distributions), therefore is a parametric q-q map that uses the theorical instead of the empirical distribution.\n#' For instance, the gamma distribution is described in Piani et al. 2010 and is applicable to precipitation. Other example is the weibull distribution, which\n#' is applicable to correct wind data (Tie et al. 2014).\n#'  \n#' \\strong{gpqm}\n#'  \n#' Generalized Quantile Mapping (described in Gutjahr and Heinemann 2013) is also a parametric quantile mapping (see\n#' method 'pqm') but using two teorethical distributions, the gamma distribution and Generalized Pareto Distribution (GPD).\n#' By default, It applies a gamma distribution to values under the threshold given by the 95th percentile \n#' (following Yang et al. 2010) and a general Pareto distribution (GPD) to values above the threshold. the threshold above \n#' which the GPD is fitted is the 95th percentile of the observed and the predicted wet-day distribution, respectively. \n#' The user can specify a different threshold by modifying the parameter theta. It is applicable to precipitation data. \n#' \n#' \n#' \\strong{variance}\n#' \n#' Variance scaling of temperature. This method is described in Chen et al. 2011. It is applicable only to temperature. It corrects\n#' the mean and variance of temperature time series.\n#' \n#' \\strong{loci}\n#' \n#' Local intensity scaling of precipitation. This method is described in Schmidli et al. 2006. It adjust the mean as well as both wet-day frequencies and wet-day intensities.\n#' The precipitation threshold is calculated such that the number of simulated days exceeding this threshold matches the number of observed days with precipitation larger than 1 mm.\n#' \n#'\\strong{ptr}\n#'\n#' Power transformation of precipitation. This method is described in Leander and Buishand 2007 and is applicable only to precipitation. It adjusts the variance statistics of precipitation\n#' time series in an exponential form. The power parameter is estimated on a monthly basis using a 90-day window centered on the interval. The power is defined by matching the coefficient\n#' of variation of corrected daily simulated precipitation with the coefficient of variation of observed daily precipitation. It is calculated by root-finding algorithm using Brent's method.\n#'\n#'\n#' @section Note on the bias correction of precipitation:\n#' \n#' In the case of precipitation a frequency adaptation has been implemented in all versions of \n#' quantile mapping to alleviate the problems arising when the dry day frequency in the raw model output is larger\n#'  than in the observations (Wilcke et al. 2013). \n#'  \n#'  The precipitation subroutines are switched-on when the variable name of the grid \n#'  (i.e., the value returned by \\code{gridData$Variable$varName}) is one of the following: \n#'  \\code{\"pr\"}, \\code{\"tp\"} (this is the standard name defined in the vocabulary (\\code{\\link[loadeR]{C4R.vocabulary}}), \\code{\"precipitation\"} or \\code{\"precip\"}.\n#'  Thus, caution must be taken to ensure that the correct bias correction is being undertaken when dealing with\n#'  non-standard variables.\n#'     \n#' \n#' @seealso \\code{\\link{isimip}} for a trend-preserving method of model calibration and \\code{\\link{quickDiagnostics}} \n#' for an outlook of the results.\n#' @return A calibrated grid of the same spatio-temporal extent than the input \\code{\"y\"}\n#' @family downscaling\n#' \n#' @importFrom transformeR redim subsetGrid getYearsAsINDEX getDim\n#' @importFrom abind adrop\n#'\n#' @references\n#'\n#' \\itemize{\n#' \\item R.A.I. Wilcke, T. Mendlik and A. Gobiet (2013) Multi-variable error correction of regional climate models. Climatic Change, 120, 871-887\n#'\n#' \\item A. Amengual, V. Homar, R. Romero, S. Alonso, and C. Ramis (2012) A Statistical Adjustment of Regional Climate Model Outputs to Local Scales: Application to Platja de Palma, Spain. J. Clim., 25, 939-957\n#'\n#' \\item C. Piani, J. O. Haerter and E. Coppola (2009) Statistical bias correction for daily precipitation in regional climate models over Europe, Theoretical and Applied Climatology, 99, 187-192\n#'\n#' \\item O. Gutjahr and G. Heinemann (2013) Comparing precipitation bias correction methods for high-resolution regional climate simulations using COSMO-CLM, Theoretical and Applied Climatology, 114, 511-529\n#' \n#' \\item M. R. Tye, D. B. Stephenson, G. J. Holland and R. W. Katz (2014) A Weibull Approach for Improving Climate Model Projections of Tropical Cyclone Wind-Speed Distributions, Journal of Climate, 27, 6119-6133\n#' \n#' }\n#' @author S. Herrera, M. Iturbide, J. Bedia\n#' @export\n#' @examples {\n#' data(\"EOBS_Iberia_pr\")\n#' data(\"CORDEX_Iberia_pr\")\n#' y <- EOBS_Iberia_pr\n#' x <- CORDEX_Iberia_pr\n#' \n#' # empirical\n#' eqm1 <- biasCorrection(y = y, x = x,\n#'                        precipitation = TRUE,\n#'                        method = \"eqm\",\n#'                        window = NULL,\n#'                        wet.threshold = 0.1,\n#'                        join.members = TRUE)\n#' eqm1win <- biasCorrection(y = y, x = x,\n#'                           precipitation = TRUE,\n#'                           method = \"eqm\",\n#'                           extrapolation = \"none\",\n#'                           window = c(30, 15),\n#'                           wet.threshold = 0.1)\n#' eqm1folds <- biasCorrection(y = y, x = x,\n#'                             precipitation = TRUE,\n#'                             method = \"eqm\",\n#'                             window = c(30, 15),\n#'                             wet.threshold = 0.1,\n#'                             cross.val = \"kfold\",\n#'                             folds = list(1983:1989, 1990:1996, 1997:2002))\n#' \n#' quickDiagnostics(y, x, eqm1, location = c(-2, 43))\n#' quickDiagnostics(y, x, eqm1win, location = c(-2, 43))\n#' quickDiagnostics(y, x, eqm1folds, location = c(-2, 43))\n#' \n#' #parametric\n#' pqm1.gamm <- biasCorrection(y = y, x = x,\n#'                        method = \"pqm\",\n#'                        precipitation = TRUE,\n#'                        fitdistr.args = list(densfun = \"gamma\"))\n#' quickDiagnostics(y, x, pqm1.gamm, location = c(-2, 43))\n#' pqm1.wei <- biasCorrection(y = y, x = x,\n#'                        method = \"pqm\",\n#'                        precipitation = TRUE,\n#'                        fitdistr.args = list(densfun = \"weibull\"))\n#' quickDiagnostics(y, x, pqm1.wei, location = c(-2, 43))\n#' \n#' data(\"EOBS_Iberia_tas\")\n#' data(\"CORDEX_Iberia_tas\")\n#' y <- EOBS_Iberia_tas\n#' x <- CORDEX_Iberia_tas\n#' pqm1.norm <- biasCorrection(y = y, x = x,\n#'            method = \"pqm\",\n#'            fitdistr.args = list(densfun = \"normal\"))\n#' quickDiagnostics(y, x, pqm1.norm, location = c(-2, 43))\n#' \n#' # correction of future climate change data\n#' data(\"CORDEX_Iberia_tas.rcp85\")\n#' newdata <- CORDEX_Iberia_tas.rcp85\n#' eqm1win <- biasCorrection(y = y, x = x,\n#'                           newdata = newdata,\n#'                           method = \"eqm\",\n#'                           extrapolation = \"constant\",\n#'                           window = c(30, 15),\n#'                           wet.threshold = 0.1)\n#' quickDiagnostics(y, x, eqm1win, location = c(-2, 43))\n#' pqm1.norm <- biasCorrection(y = y, x = x,\n#'                        newdata = newdata,\n#'                        method = \"pqm\",\n#'                        fitdistr.args = list(densfun = \"normal\"))\n#' quickDiagnostics(y, x, pqm1.norm, location = c(-2, 43))\n#' \n#' # Correction of multimember datasets considering the joint\n#' # distribution of all members\n#' data(\"EOBS_Iberia_pr\")\n#' data(\"CFS_Iberia_pr\")\n#' y <- EOBS_Iberia_pr\n#' x <- CFS_Iberia_pr\n#' eqm.join <- biasCorrection(y = y, x = x,\n#'                            precipitation = TRUE,\n#'                            method = \"eqm\",\n#'                            window = NULL,\n#'                            wet.threshold = 0.1,\n#'                            join.members = TRUE)\n#' }\n\n\n\nbiasCorrection <- function(y, x, newdata = NULL, precipitation = FALSE,\n                           method = c(\"delta\", \"scaling\", \"eqm\", \"pqm\", \"gpqm\", \"loci\"),\n                           cross.val = c(\"none\", \"loo\", \"kfold\"),\n                           folds = NULL,\n                           window = NULL,\n                           scaling.type = c(\"additive\", \"multiplicative\"),\n                           fitdistr.args = list(densfun = \"normal\"),\n                           wet.threshold = 1,\n                           n.quantiles = NULL,\n                           extrapolation = c(\"none\", \"constant\"), \n                           theta = .95,\n                           join.members = FALSE,\n                           parallel = FALSE,\n                           max.ncores = 16,\n                           ncores = NULL) {\n      if (method == \"gqm\") stop(\"'gqm' is not a valid choice anymore. Use method = 'pqm' instead and set fitdistr.args = list(densfun = 'gamma')\")\n      method <- match.arg(method, choices = c(\"delta\", \"scaling\", \"eqm\", \"pqm\", \"gpqm\", \"loci\", \"ptr\", \"variance\"))\n      cross.val <- match.arg(cross.val, choices = c(\"none\", \"loo\", \"kfold\"))\n      scaling.type <- match.arg(scaling.type, choices = c(\"additive\", \"multiplicative\"))\n      extrapolation <- match.arg(extrapolation, choices = c(\"none\", \"constant\"))\n      stopifnot(is.logical(join.members))\n      nwdatamssg <- TRUE\n      if (is.null(newdata)) {\n            newdata <- x \n            nwdatamssg <- FALSE\n      }\n      if (cross.val == \"none\") {\n            output <- biasCorrectionXD(y = y, x = x, newdata = newdata, \n                                       precipitation = precipitation,\n                                       method = method,\n                                       window = window,\n                                       scaling.type = scaling.type,\n                                       fitdistr.args = fitdistr.args,\n                                       pr.threshold = wet.threshold, \n                                       n.quantiles = n.quantiles, \n                                       extrapolation = extrapolation, \n                                       theta = theta,\n                                       join.members = join.members,\n                                       parallel = parallel,\n                                       max.ncores = max.ncores,\n                                       ncores = ncores)\n      } else {\n            if (nwdatamssg) {\n                  message(\"'newdata' will be ignored for cross-validation\")\n            }\n            if (cross.val == \"loo\") {\n                  years <- as.list(unique(getYearsAsINDEX(x)))\n            } else if (cross.val == \"kfold\" & !is.null(folds)) {\n                  years <- folds\n            } else if (cross.val == \"kfold\" & is.null(folds)) {\n                  stop(\"Fold specification is missing, with no default\")\n            }\n            output.list <- lapply(1:length(years), function(i) {\n                  target.year <- years[[i]]\n                  rest.years <- setdiff(unlist(years), target.year)\n                  station <- FALSE\n                  if (\"loc\" %in% getDim(y)) station <- TRUE\n                  yy <- redim(y, member = FALSE)\n                  yy <- if (method == \"delta\") {\n                        subsetGrid(yy, years = target.year, drop = FALSE)\n                  } else {\n                        subsetGrid(yy, years = rest.years, drop = FALSE)\n                  }\n                  if (isTRUE(station)) {\n                        yy$Data <- adrop(yy$Data, drop = 3)\n                        attr(yy$Data, \"dimensions\") <- c(setdiff(getDim(yy), c(\"lat\", \"lon\")), \"loc\")\n                  } else {\n                        yy <- redim(yy, drop = TRUE)\n                  }\n                  newdata2 <- subsetGrid(x, years = target.year, drop = F)\n                  xx <- subsetGrid(x, years = rest.years, drop = F)\n                  message(\"Validation \", i, \", \", length(unique(years)) - i, \" remaining\")\n                  biasCorrectionXD(y = yy, x = xx, newdata = newdata2, precipitation = precipitation,\n                                   method = method,\n                                   window = window,\n                                   scaling.type = scaling.type,\n                                   fitdistr.args = fitdistr.args,\n                                   pr.threshold = wet.threshold, n.quantiles = n.quantiles, extrapolation = extrapolation, \n                                   theta = theta, join.members = join.members,\n                                   parallel = parallel,\n                                   max.ncores = max.ncores,\n                                   ncores = ncores)\n            })\n            al <- which(getDim(x) == \"time\")\n            Data <- sapply(output.list, function(n) unname(n$Data), simplify = FALSE)\n            bindata <- unname(do.call(\"abind\", c(Data, along = al)))\n            output <- output.list[[1]]\n            dimNames <- attr(output$Data, \"dimensions\")\n            output$Data <- bindata\n            attr(output$Data, \"dimensions\") <- dimNames\n            output$Dates <- x$Dates\n      }\n      return(output)\n}\n\n\n#' @keywords internal\n#' @importFrom transformeR redim subsetGrid getDim\n\nbiasCorrectionXD <- function(y, x, newdata, \n                             precipitation, \n                             method,\n                             window,\n                             scaling.type,\n                             fitdistr.args,\n                             pr.threshold, \n                             n.quantiles, \n                             extrapolation, \n                             theta,\n                             join.members,\n                             parallel = FALSE,\n                             max.ncores = 16,\n                             ncores = NULL) {\n      station <- FALSE\n      if (\"loc\" %in% getDim(y)) station <- TRUE\n      xy <- y$xyCoords\n      suppressWarnings(suppressMessages(pred <- interpGrid(x, getGrid(y))))\n      suppressWarnings(suppressMessages(sim <- interpGrid(newdata, getGrid(y))))\n      delta.method <- method == \"delta\"\n      precip <- precipitation\n      message(\"[\", Sys.time(), \"] Argument precipitation is set as \", precip, \", please ensure that this matches your data.\")\n      bc <- y\n      if (isTRUE(join.members) & getShape(redim(sim))[\"member\"] > 1) {\n            n.mem.aux <- getShape(sim)[\"member\"]\n            pred <- flatMemberDim(pred, station)\n            pred <- redim(pred, drop = T)\n            sim <- flatMemberDim(sim, station)\n            sim <- redim(sim, drop = T)\n            y <- bindGrid(rep(list(y), n.mem.aux), dimension = \"time\")\n      } else if (isTRUE(join.members) & !getShape(redim(sim))[\"member\"] > 1) {\n            warning(\"There is only one member, argument join.members ignored.\")\n            join.members <- FALSE\n      }\n      y <- redim(y, drop = TRUE)\n      y <- redim(y, member = FALSE, runtime = FALSE)\n      pred <- redim(pred, member = TRUE, runtime = TRUE)\n      sim <- redim(sim, member = TRUE, runtime = TRUE)\n      dimNames <- attr(y$Data, \"dimensions\")\n      n.run <- getShape(sim)[\"runtime\"]\n      n.mem <- getShape(sim)[\"member\"]\n      if (join.members & !is.null(window)) {\n            message(\"[\", Sys.time(), \"] Window option is currently not supported for joined members and will be ignored\")\n            window <- NULL\n      }\n      if (!is.null(window)) {\n            win <- getWindowIndex(y = y, x = pred, newdata = sim, window = window, delta.method = delta.method)\n      } else {\n            win <- list()\n            indobservations <- match(as.POSIXct(pred$Dates$start), as.POSIXct(y$Dates$start))\n            ## esto no mola, es para el caso especial de join members...hay que mirarlo\n            if (length(indobservations) > length(unique(indobservations))) indobservations <- 1:length(indobservations) \n            win[[\"Window1\"]] <- list(\"obsWindow\" = indobservations, \"window\" = 1:getShape(pred)[\"time\"], \"step\" = 1:getShape(sim)[\"time\"])\n            if (delta.method) win[[\"Window1\"]][[\"deltaind\"]] <- indobservations\n      }\n      message(\"[\", Sys.time(), \"] Number of windows considered: \", length(win), \"...\")\n      winarr <- array(dim = dim(sim$Data))\n      if (delta.method) winarr <- array(dim = c(n.run, n.mem, getShape(y)))\n      for (j in 1:length(win)) {\n            yind <- win[[j]]$obsWindow\n            outind <- win[[j]]$step\n            if (delta.method) {\n                  yind <- win[[j]]$deltaind\n                  outind <- win[[j]]$deltaind\n            } \n            yw <- y$Data[yind,,, drop = FALSE]\n            pw <- pred$Data[,,win[[j]]$window,,, drop = FALSE]\n            sw <- sim$Data[,,win[[j]]$step,,, drop = FALSE]\n            runarr <- lapply(1:n.run, function(l){\n                  memarr <- lapply(1:n.mem, function(m){\n                        #join members message\n                        if (j == 1 & m == 1) {\n                              if (!isTRUE(join.members)) {\n                                    message(\"[\", Sys.time(), \"] Bias-correcting \", n.mem, \" members separately...\")\n                              } else {\n                                    message(\"[\", Sys.time(), \"] Bias-correcting \", attr(pred, \"orig.mem.shape\"), \" members considering their joint distribution...\")\n                              }\n                        }\n                        o = yw[, , , drop = FALSE]\n                        p = adrop(pw[l, m, , , , drop = FALSE], drop = c(T, T, F, F, F))\n                        s = adrop(sw[l, m, , , , drop = FALSE], drop = c(T, T, F, F, F))\n                        data <- list(o, p, s)\n                        if (!station) {\n                              data <- lapply(1:length(data), function(x) {\n                                    attr(data[[x]], \"dimensions\") <- dimNames\n                                    abind(array3Dto2Dmat(data[[x]]), along = 3)\n                              }) \n                        }\n                        o <- lapply(seq_len(ncol(data[[1]])), function(i) data[[1]][,i,1])\n                        p <- lapply(seq_len(ncol(data[[2]])), function(i) data[[2]][,i,1])\n                        s <- lapply(seq_len(ncol(data[[3]])), function(i) data[[3]][,i,1])\n                        mat <- biasCorrection1D(o, p, s,\n                                                method = method,\n                                                scaling.type = scaling.type,\n                                                fitdistr.args = fitdistr.args,\n                                                precip = precip,\n                                                pr.threshold = pr.threshold,\n                                                n.quantiles = n.quantiles,\n                                                extrapolation = extrapolation,\n                                                theta = theta,\n                                                parallel = parallel,\n                                                max.ncores = max.ncores,\n                                                ncores = ncores)  \n                        if (!station) mat <- mat2Dto3Darray(mat, xy$x, xy$y)\n                        mat\n                  })\n                  unname(do.call(\"abind\", list(memarr, along = 0)))\n            })\n            winarr[,,outind,,] <- unname(do.call(\"abind\", list(runarr, along = 0))) \n      }\n      bc$Data <- unname(do.call(\"abind\", list(winarr, along = 3)))\n      attr(bc$Data, \"dimensions\") <- attr(sim$Data, \"dimensions\")\n      if (station) bc <- redim(bc, loc = TRUE)\n      bc$Dates <- sim$Dates\n      ## Recover the member dimension when join.members=TRUE:\n      if (isTRUE(join.members)) {\n            if (method == \"delta\") {\n                  bc <- recoverMemberDim(plain.grid = pred, bc.grid = bc, newdata = newdata)\n            }else{\n                  bc <- recoverMemberDim(plain.grid = sim, bc.grid = bc, newdata = newdata)      \n            }\n      } else {\n            bc$InitializationDates <- sim$InitializationDates\n            bc$Members <- sim$Members\n      }\n      attr(bc$Variable, \"correction\") <- method\n      bc <- redim(bc, drop = TRUE)\n      message(\"[\", Sys.time(), \"] Done.\")\n      return(bc)\n}\n\n\n#' @title Get the index of the window days.\n#' @description Get the index of the days that corresponding to the window and the target days centered in it.\n#' \n#' @param y A grid or station data containing the observed climate data for the training period\n#' @param newdata A grid containing the simulated climate for the test period.\n#' @param method method applied. Current accepted values are \\code{\"eqm\"}, \\code{\"delta\"},\n#'  \\code{\"scaling\"}, \\code{\"pqm\"} and \\code{\"gpqm\"} \\code{\"variance\"},\\code{\"loci\"} and \\code{\"ptr\"}. See details.\n#' @param window vector of length = 2 (or 1) specifying the time window width used to calibrate and the \n#' target days (days that are being corrected). If the window length = 1 the window width is no larger than the \n#' target days. The window is centered on the target day/s (window width >= target days). \n#' @param delta.method Logical (default is FALSE).\n#' @keywords internal\n#' @author M. Iturbide\n\ngetWindowIndex <- function(y, x, newdata, window, delta.method = FALSE){\n      if (length(window) == 1) window <- rep(window, 2)\n      step <- window[2]\n      window <- window[1]\n      if (window - step < 0) stop(\"The first argument of window must be equal or higher than the second. See ?biasCorrection\")\n      datesList <- as.POSIXct(x$Dates$start, tz = \"GMT\", format = \"%Y-%m-%d\")\n      yearList <- unlist(strsplit(as.character(datesList), \"[-]\"))\n      dayListObs <- array(data = c(as.numeric(yearList[seq(2,length(yearList),3)]),as.numeric(yearList[seq(3,length(yearList),3)])), dim = c(length(datesList),2))\n      dayList <- unique(dayListObs,index.return = datesList)\n      annual <- TRUE\n      if (nrow(dayList) < 360) annual <- FALSE\n      indDays <- array(data = NaN, dim = c(length(datesList),1))\n      for (d in 1:dim(dayList)[1]) {\n            indDays[which(sqrt((dayListObs[,1] - dayList[d,1]) ^ 2 + (dayListObs[,2] - dayList[d,2]) ^ 2) == 0)] <- d\n      }\n      datesList <- as.POSIXct(newdata$Dates$start, tz = \"GMT\", format = \"%Y-%m-%d\")\n      yearList <- unlist(strsplit(as.character(datesList), \"[-]\"))\n      dayListSim <- array(data = c(as.numeric(yearList[seq(2,length(yearList),3)]),as.numeric(yearList[seq(3,length(yearList),3)])), dim = c(length(datesList),2))\n      indDaysSim <- array(data = NaN, dim = c(length(datesList),1))\n      for (d in 1:dim(dayList)[1]) {\n            indDaysSim[which(sqrt((dayListSim[,1] - dayList[d,1]) ^ 2 + (dayListSim[,2] - dayList[d,2]) ^ 2) == 0)] <- d\n      }\n      steps <- floor(dim(dayList)[1]/step)\n      #steps loop\n      output <- list()\n      for (j in 1:steps) {\n            days <- ((j - 1) * step + 1):((j - 1) * step + step)\n            if (j == steps) days <- days[1]:dim(dayList)[1]\n            indObs <- lapply(1:length(days), function(h){\n                  which(indDays == days[h])\n            })\n            indObs <- sort(do.call(\"abind\", indObs))\n            head <- floor((window - step)/2)\n            tail <- head\n            before <- after <- FALSE\n            if (!annual) {\n                  before <- min(indDays[indObs]) - 1 - head < 1\n                  if (before) head <- head + (min(indDays[indObs]) - 1 - head) \n                  after <- max(indDays[indObs]) + tail > nrow(dayList)\n                  if (after) tail <- nrow(dayList) - max(indDays[indObs])   \n            }\n            indObsWindow <- array(data = NA, dim = c((head + step + tail)*length(indObs)/step,1))\n            breaks <- c(which(diff(indObs) != 1), length(indObs))\n            for (d in 1:length(breaks)) {\n                  if (d == 1) {\n                        piece <- indObs[1:breaks[1]]\n                  } else {\n                        piece <- indObs[(breaks[d - 1] + 1):breaks[d]]\n                  }\n                  suppressWarnings(indObsWindow[((d - 1) * (head + step + tail) + 1):(d * (head + step + tail))] <- \n                                         (min(piece, na.rm = TRUE) - head):(max(piece, na.rm = TRUE) + tail))\n            }\n            if (annual) {\n                  indObsWindow[which(indObsWindow <= 0)] <- 1\n                  indObsWindow[which(indObsWindow >  length(indDays))] <- length(indDays)\n                  indObsWindow <- unique(indObsWindow)\n            }\n            indSim <- lapply(1:length(days), function(h){\n                  which(indDaysSim == days[h])\n            })\n            indSim <- sort(do.call(\"abind\", indSim))\n            names(indSim) <- newdata$Dates$start[indSim]\n            indObsWindow <- indObsWindow[which(!is.na(x$Dates$start[indObsWindow]))]\n            names(indObsWindow) <- x$Dates$start[indObsWindow]\n            indobservations <- match(as.POSIXct(x$Dates$start[indObsWindow], format = \"%Y-%m-%d\"), as.POSIXct(y$Dates$start, format = \"%Y-%m-%d\"))\n            names(indobservations) <- y$Dates$start[indobservations]\n            names(indObs) <- x$Dates$start[indObs]\n            indObsObs <- match(as.POSIXct(x$Dates$start[indObs], format = \"%Y-%m-%d\"), as.POSIXct(y$Dates$start, format = \"%Y-%m-%d\"))\n            output[[paste0(\"Window\", j)]] <- list(\"obsWindow\" = indobservations, \"window\" = indObsWindow, \"step\" = indSim)\n            if (delta.method) output[[paste0(\"Window\", j)]][[\"deltaind\"]] <- indObsObs\n      }\n      return(output)\n}\n\n\n#' @title Bias correction methods on 1D data\n#' @description Implementation of several standard bias correction methods\n#' @param o A vector (e.g. station data) containing the observed climate data for the training period\n#' @param p A vector containing the simulated climate by the model for the training period. \n#' @param s A vector containing the simulated climate for the variable used in \\code{p}, but considering the test period.\n#' @param method method applied. Current accepted values are \\code{\"eqm\"}, \\code{\"delta\"},\n#'  \\code{\"scaling\"}, \\code{\"pqm\"} and \\code{\"gpqm\"} \\code{\"variance\"},\\code{\"loci\"} and \\code{\"ptr\"}. See details in \n#'  function \\code{\\link{biasCorrection}}.\n#' @param scaling.type Character indicating the type of the scaling method. Options are \\code{\"additive\"} \n#' or \\code{\"multiplicative\"} (see details). This argument is ignored if \\code{\"scaling\"} is not \n#' selected as the bias correction method.\n#' @param  fitdistr.args Further arguments passed to function \\code{\\link[MASS]{fitdistr}} \n#' (\\code{densfun}, \\code{start}, \\code{...}). Only used when applying the \"pqm\" method \n#' (parametric quantile mapping). Please, read the \\code{\\link[MASS]{fitdistr}} help \n#' document  carefully before setting the parameters in \\code{fitdistr.args}.\n#' @param precip Logical for precipitation data. If TRUE Adjusts precipitation \n#' frequency in 'x' (prediction) to the observed frequency in 'y'. This is a preprocess to bias correct \n#' precipitation data following ThemeÃŸl et al. (2012). To adjust the frequency, \n#' parameter \\code{pr.threshold} is used (see below).\n#' @param pr.threshold The minimum value that is considered as a non-zero precipitation. Ignored when \n#' \\code{precip = FALSE}. See details in function \\code{biasCorrection}.\n#' @param n.quantiles Integer indicating the number of quantiles to be considered when method = \"eqm\". \n#' @param extrapolation Character indicating the extrapolation method to be applied to correct values in  \n#' \\code{newdata} that are out of the range of \\code{x}. Extrapolation is applied only to the \\code{\"eqm\"} method, \n#' thus, this argument is ignored if other bias correction method is selected.\n#' @param theta numeric indicating  upper threshold (and lower for the left tail of the distributions, if needed) \n#' above which precipitation (temperature) values are fitted to a Generalized Pareto Distribution (GPD). \n#' Values below this threshold are fitted to a gamma (normal) distribution. By default, 'theta' is the 95th \n#' percentile (5th percentile for the left tail). Only for \\code{\"gpqm\"} method.\n#' @template templateParallelParams\n#' \n#' \n#' @importFrom transformeR parallelCheck selectPar.pplyFun\n#' @keywords internal\n#' @author M. Iturbide\n\nbiasCorrection1D <- function(o, p, s,\n                             method, \n                             scaling.type,\n                             fitdistr.args,\n                             precip, \n                             pr.threshold,\n                             n.quantiles,\n                             extrapolation, \n                             theta,\n                             parallel = FALSE,\n                             max.ncores = 16,\n                             ncores = NULL) {\n      parallel.pars <- parallelCheck(parallel, max.ncores, ncores)\n      mapply_fun <- selectPar.pplyFun(parallel.pars, .pplyFUN = \"mapply\")\n      if (parallel.pars$hasparallel) on.exit(parallel::stopCluster(parallel.pars$cl))\n      if (method == \"delta\") {\n            mapply_fun(delta, o, p, s)\n      } else if (method == \"scaling\") {\n            mapply_fun(scaling, o, p, s, MoreArgs = list(scaling.type = scaling.type))\n      } else if (method == \"eqm\") {\n            suppressWarnings(\n                  mapply_fun(eqm, o, p, s, MoreArgs = list(precip, pr.threshold, n.quantiles, extrapolation))\n            )\n      } else if (method == \"pqm\") {\n            mapply_fun(pqm, o, p, s, MoreArgs = list(fitdistr.args, precip, pr.threshold))\n      } else if (method == \"gpqm\") {\n            mapply_fun(gpqm, o, p, s, MoreArgs = list(precip, pr.threshold, theta))\n      } else if (method == \"variance\") {\n            mapply_fun(variance, o, p, s, MoreArgs = list(precip))\n      } else if (method == \"loci\") {\n            mapply_fun(loci, o, p, s, MoreArgs = list(precip, pr.threshold))\n      } else if (method == \"ptr\") {\n            mapply_fun(ptr, o, p, s, MoreArgs = list(precip))\n      }\n}\n\n#' @title adjustPrecipFreq\n#' @description Adjusts precipitation frequency in 'p' (prediction) to the observed frequency in 'o'. \n#' It constitutes a preprocess to bias correct precipitation data following ThemeÃŸl et al. (2012). \n#' @param obs A vector (e.g. station data) containing the observed climate data for the training period\n#' @param pred A vector containing the simulated climate by the model for the training period. \n#' @param threshold The minimum value that is considered as a non-zero precipitation. \n#' @importFrom MASS fitdistr\n#' @keywords internal\n#' @importFrom stats rgamma\n#' @author S. Herrera and M. Iturbide\n\nadjustPrecipFreq <- function(obs, pred, threshold){\n      o <- obs[!is.na(obs)]\n      p <- pred[!is.na(pred)]\n      # Number of dry days in 'o' \n      nPo <- sum(as.double(o < threshold))\n      # Number of dry days that must be in 'p' to equal precip frequency in 'o'\n      nPp <- ceiling(length(p) * nPo / length(o))\n      # Index and values of ordered 'p'\n      ix <- sort(p, decreasing = FALSE, index.return = TRUE)$ix\n      Ps <- sort(p, decreasing = FALSE)\n      Pth <- max(Ps[nPp:(nPp + 1)], na.rm = TRUE) # in case nPp == length(Ps)\n      # ThemeÃŸl (Themessl) modification (simulating rain for model dry days) \n      inddrzl <- which(Ps[(nPp + 1):length(Ps)] < threshold)\n      if (length(inddrzl) > 0) { \n            Os <- sort(o, decreasing = FALSE, na.last = NA)\n            indO <- ceiling(length(Os) * (nPp + max(inddrzl))/length(Ps))\n            auxOs <- Os[(nPo + 1):indO]\n            if (length(unique(auxOs)) > 6) {\n                  # simulate precip for 'p' with a gamma adjusted in 'o' for values between\n                  auxGamma <- fitdistr(auxOs, \"gamma\")\n                  Ps[(nPp + 1):(nPp + max(inddrzl))] <- rgamma(length(inddrzl), auxGamma$estimate[1], rate = auxGamma$estimate[2])\n            } else {\n                  Ps[(nPp + 1):(nPp + max(inddrzl))] <- mean(auxOs)\n            }\n            # order 'Ps' after simulation\n            Ps <- sort(Ps, decreasing = FALSE, na.last = NA)\n      }\n      # Make 0-s\n      if (nPo > 0) {\n            ind <- min(nPp, length(p))\n            Ps[1:ind] <- 0\n      }\n      p[ix] <- Ps\n      pred[!is.na(pred)] <- p\n      return(list(\"nP\" = c(nPo,nPp), \"Pth\" = Pth, \"p\" = pred)) \n}\n#end\n\n#' @title Delta method for bias correction\n#' @description Implementation of Delta method for bias correction \n#' @param o A vector (e.g. station data) containing the observed climate data for the training period\n#' @param p A vector containing the simulated climate by the model for the training period. \n#' @param s A vector containing the simulated climate for the variable used in \\code{x}, but considering the test period.\n#' @keywords internal\n#' @author S. Herrera and M. Iturbide\n\ndelta <- function(o, p, s){\n      corrected <- o + (mean(s) - mean(p))\n      return(corrected)\n}\n\n\n#' @title Scaling method for bias correction\n#' @description Implementation of Scaling method for bias correction \n#' @param o A vector (e.g. station data) containing the observed climate data for the training period\n#' @param p A vector containing the simulated climate by the model for the training period. \n#' @param s A vector containing the simulated climate for the variable used in \\code{x}, but considering the test period.\n#' @param scaling.type Character indicating the type of the scaling method. Options are \\code{\"additive\"} (default)\n#' or \\code{\"multiplicative\"} (see details). This argument is ignored if \\code{\"scaling\"} is not selected as the bias correction method.\n#' @keywords internal\n#' @author S. Herrera and M. Iturbide\n\nscaling <- function(o, p, s, scaling.type){\n      if (scaling.type == \"additive\") {\n            s - mean(p) + mean(o, na.rm = TRUE)\n      } else if (scaling.type == \"multiplicative\") {\n            (s/mean(p)) * mean(o, na.rm = TRUE)\n      }\n}\n\n\n\n#' @title Parametric Quantile Mapping method for bias correction\n#' @description Implementation of Parametric Quantile Mapping method for bias correction \n#' @param o A vector (e.g. station data) containing the observed climate data for the training period\n#' @param p A vector containing the simulated climate by the model for the training period. \n#' @param s A vector containing the simulated climate for the variable used in \\code{x}, but considering the test period.\n#' @param  fitdistr.args Further arguments passed to function \\code{\\link[MASS]{fitdistr}} \n#' (\\code{densfun}, \\code{start}, \\code{...}). Only used when applying the \"pqm\" method \n#' (parametric quantile mapping). Please, read the \\code{\\link[MASS]{fitdistr}} help \n#' document  carefully before parameter setting in \\code{fitdistr.args}.\n#' @param precip Logical for precipitation data. If TRUE Adjusts precipitation \n#' frequency in 'x' (prediction) to the observed frequency in 'y'. This is a preprocess to bias correct \n#' precipitation data following ThemeÃŸl et al. (2012). To adjust the frequency, \n#' parameter \\code{pr.threshold} is used (see below).\n#' @param pr.threshold The minimum value that is considered as a non-zero precipitation. Ignored when \n#' \\code{precip = FALSE}. See details in function \\code{biasCorrection}.\n#' @importFrom MASS fitdistr\n#' @importFrom stats pgamma qgamma \n#' @keywords internal\n#' @author S. Herrera and M. Iturbide\n\npqm <- function(o, p, s, fitdistr.args, precip, pr.threshold){\n      dfdistr <- cbind(\"df\" = c( \"beta\", \"cauchy\", \"chi-squared\", \"exponential\", \"f\", \"gamma\", \"geometric\", \"log-normal\", \"lognormal\", \"logistic\", \"negative binomial\", \"normal\", \"Poisson\", \"t\", \"weibull\"),\n                        \"p\" = c(\"pbeta\", \"pcauchy\", \"pchisq\", \"pexp\", \"pf\", \"pgamma\", \"pegeom\", \"plnorm\", \"plnorm\", \"plogis\", \"pnbinom\", \"pnorm\", \"ppois\", \"pt\", \"pweibull\"),\n                        \"q\" = c(\"qbeta\", \"qcauchy\", \"qchisq\", \"qexp\", \"qf\", \"qgamma\", \"qegeom\", \"qlnorm\", \"qlnorm\", \"qlogis\", \"qnbinom\", \"qnorm\", \"qpois\", \"qt\", \"qweibull\"))\n      fitdistr.args <- fitdistr.args[which(names(fitdistr.args) != \"x\")]\n      statsfunp <- unname(dfdistr[which(dfdistr[,\"df\"] == fitdistr.args$densfun), \"p\"])\n      statsfunq <- unname(dfdistr[which(dfdistr[,\"df\"] == fitdistr.args$densfun), \"q\"])\n      run <- TRUE\n      ind.o <- 1:length(o)\n      ind.p <- 1:length(p)\n      rain <- 1:length(s)\n      if (precip) {\n            threshold <- pr.threshold\n            if (any(!is.na(o))) {\n                  params <-  adjustPrecipFreq(o, p, threshold)\n                  p <- params$p\n                  nP <- params$nP\n                  Pth <- params$Pth\n            } else {\n                  nP = NULL\n            }\n            if (is.null(nP)) {\n                  run <- FALSE\n                  s <- rep(NA, length(s))\n            } else if (nP[1] < length(o)) {\n                  ind.o <- which(o > threshold & !is.na(o))\n                  ind.p <- which(p > 0 & !is.na(p))\n                  rain <- which(s > Pth & !is.na(s))\n                  noRain <- which(s <= Pth & !is.na(s))\n            } else {\n                  run <- FALSE\n                  warning(\"For the window step selected, location without rainfall above the threshold.\\n no bias correction applied in location.\")\n            } \n      }\n      if (all(is.na(o[ind.o]))) {\n            run <- FALSE\n            s <- rep(NA, length(s))\n      }\n      if (run) {\n            fitdistr.args.o <- c(\"x\" = list(o[ind.o]), fitdistr.args)\n            fitdistr.args.p <- c(\"x\" = list(p[ind.p]), fitdistr.args)\n            obsGamma <- tryCatch({do.call(\"fitdistr\", fitdistr.args.o)}, error = function(err){NULL})\n            prdGamma <- tryCatch({do.call(\"fitdistr\", fitdistr.args.p)}, error = function(err){NULL})\n            if (!is.null(prdGamma) & !is.null(obsGamma)) {\n                  statsfun.args <- c(list(s[rain]), as.list(prdGamma$estimate))\n                  auxF <- do.call(statsfunp, statsfun.args)\n                  statsfun.args <- c(list(auxF), as.list(obsGamma$estimate))\n                  s[rain] <- do.call(statsfunq, statsfun.args)\n                  if (precip) s[noRain] <- 0\n            } else {\n                  warning(\"Fitting error for location and selected 'densfun'.\")\n            }\n      }   \n      return(s)      \n}   \n#end\n\n#' @title Empirical Quantile Mapping method for bias correction\n#' @description Implementation of Empirical Quantile Mapping method for bias correction \n#' @param o A vector (e.g. station data) containing the observed climate data for the training period\n#' @param p A vector containing the simulated climate by the model for the training period. \n#' @param s A vector containing the simulated climate for the variable used in \\code{p}, but considering the test period.\n#' @param precip Logical for precipitation data. If TRUE Adjusts precipitation \n#' frequency in 'x' (prediction) to the observed frequency in 'y'. This is a preprocess to bias correct \n#' precipitation data following ThemeÃŸl et al. (2012). To adjust the frequency, \n#' parameter \\code{pr.threshold} is used (see below).\n#' @param pr.threshold The minimum value that is considered as a non-zero precipitation. Ignored when \n#' \\code{precip = FALSE}. See details in function \\code{biasCorrection}.\n#' @param n.quantiles Integer indicating the number of quantiles to be considered when method = \"eqm\". Default is NULL, \n#' that considers all quantiles, i.e. \\code{n.quantiles = length(p)}.\n#' @param extrapolation Character indicating the extrapolation method to be applied to correct values in  \n#' \\code{\"s\"} that are out of the range of \\code{\"p\"}. Extrapolation is applied only to the \\code{\"eqm\"} method, \n#' thus, this argument is ignored if other bias correction method is selected. \n#' @keywords internal\n#' @importFrom stats approxfun ecdf quantile\n#' @author S. Herrera and M. Iturbide\n\neqm <- function(o, p, s, precip, pr.threshold, n.quantiles, extrapolation){\n      if (precip == TRUE) {\n            threshold <- pr.threshold\n            if (any(!is.na(o))) {\n                  params <-  adjustPrecipFreq(o, p, threshold)\n                  p <- params$p\n                  nP <- params$nP\n                  Pth <- params$Pth\n            } else {\n                  nP = NULL\n            }\n            smap <- rep(NA, length(s))\n            if (any(!is.na(p)) & any(!is.na(o))) {\n                  if (length(which(p > Pth)) > 0) { \n                        noRain <- which(s <= Pth & !is.na(s))\n                        rain <- which(s > Pth & !is.na(s))\n                        drizzle <- which(s > Pth  & s  <= min(p[which(p > Pth)], na.rm = TRUE) & !is.na(s))\n                        if (length(rain) > 0) {\n                              eFrc <- tryCatch({ecdf(s[rain])}, error = function(err) {stop(\"There are not precipitation days in newdata for the step length selected in one or more locations. Try to enlarge the window step\")})\n                              if (is.null(n.quantiles)) n.quantiles <- length(p)\n                              bins <- n.quantiles\n                              qo <- quantile(o[which(o > threshold & !is.na(o))], prob = seq(1/bins,1 - 1/bins,1/bins), na.rm = T)\n                              qp <- quantile(p[which(p > Pth)], prob = seq(1/bins,1 - 1/bins,1/bins), na.rm = T)\n                              p2o <- tryCatch({approxfun(qp, qo, method = \"linear\")}, error = function(err) {NA})\n                              smap <- s\n                              smap[rain] <- if (suppressWarnings(!is.na(p2o))) {\n                                    p2o(s[rain])\n                              }else{\n                                    s[rain] <- NA\n                              }\n                              # Linear extrapolation was discarded due to lack of robustness \n                              if (extrapolation == \"constant\") {\n                                    smap[rain][which(s[rain] > max(qp, na.rm = TRUE))] <- s[rain][which(s[rain] > max(qp, na.rm = TRUE))] + (qo[length(qo)] - qp[length(qo)])\n                                    smap[rain][which(s[rain] < min(qp, na.rm = TRUE))] <- s[rain][which(s[rain] < min(qp, na.rm = TRUE))] + (qo[1] - qp[1]) \n                              } else {\n                                    smap[rain][which(s[rain] > max(qp, na.rm = TRUE))] <- qo[length(qo)]\n                                    smap[rain][which(s[rain] < min(qp, na.rm = TRUE))] <- qo[1]\n                              }\n                        }else{\n                              smap <- rep(0, length(s))\n                              warning(\"There are not precipitation days in newdata for the step length selected in one or more locations. Consider the possibility of enlarging the window step\")\n                        }\n                        if (length(drizzle) > 0) {\n                              smap[drizzle] <- quantile(s[which(s > min(p[which(p > Pth)], na.rm = TRUE) & !is.na(s))], probs = eFrc(s[drizzle]), na.rm = TRUE, type = 4)\n                        }\n                        smap[noRain] <- 0\n                  } else { ## For dry series\n                        smap <- s\n                        warning('No rainy days in the prediction. Bias correction is not applied') \n                  }\n            }\n      } else {\n            if (all(is.na(o))) {\n                  smap <- rep(NA, length(s))\n            } else if (all(is.na(p))) {\n                  smap <- rep(NA, length(s))\n            }else if (any(!is.na(p)) & any(!is.na(o))) {\n                  if (is.null(n.quantiles)) n.quantiles <- length(p)\n                  bins <- n.quantiles\n                  qo <- quantile(o, prob = seq(1/bins,1 - 1/bins,1/bins), na.rm = TRUE)\n                  qp <- quantile(p, prob = seq(1/bins,1 - 1/bins,1/bins), na.rm = TRUE)\n                  p2o <- approxfun(qp, qo, method = \"linear\")\n                  smap <- p2o(s)\n                  if (extrapolation == \"constant\") {\n                        smap[which(s > max(qp, na.rm = TRUE))] <- s[which(s > max(qp, na.rm = TRUE))] + (qo[length(qo)] - qp[length(qo)])\n                        smap[which(s < min(qp, na.rm = TRUE))] <- s[which(s < min(qp, na.rm = TRUE))] + (qo[1] - qp[1]) \n                  } else {\n                        smap[which(s > max(qp, na.rm = TRUE))] <- qo[length(qo)]\n                        smap[which(s < min(qp, na.rm = TRUE))] <- qo[1]\n                  }\n            } \n      }\n      return(smap)\n}\n#end\n\n#' @title Generalized Quantile Mapping method for bias correction\n#' @description Implementation of Generalized Quantile Mapping method for bias correction \n#' @param o A vector (e.g. station data) containing the observed climate data for the training period\n#' @param p A vector containing the simulated climate by the model for the training period. \n#' @param s A vector containing the simulated climate for the variable used in \\code{p}, but considering the test period.\n#' @param precip Logical for precipitation data. If TRUE Adjusts precipitation \n#' frequency in 'x' (prediction) to the observed frequency in 'y'. This is a preprocess to bias correct \n#' precipitation data following ThemeÃŸl et al. (2012). To adjust the frequency, \n#' parameter \\code{pr.threshold} is used (see below).\n#' @param pr.threshold The minimum value that is considered as a non-zero precipitation. Ignored when \n#' \\code{precip = FALSE}. See details in function \\code{biasCorrection}.\n#' @param theta numeric indicating  upper threshold (and lower for the left tail of the distributions, if needed) \n#' above which precipitation (temperature) values are fitted to a Generalized Pareto Distribution (GPD). \n#' Values below this threshold are fitted to a gamma (normal) distribution. By default, 'theta' is the 95th \n#' percentile (5th percentile for the left tail). \n#' @importFrom evd fpot\n#' @importFrom MASS fitdistr\n#' @importFrom evd qgpd pgpd\n#' @importFrom stats quantile pgamma qgamma\n#' @keywords internal\n#' @author S. Herrera and M. Iturbide\n\ngpqm <- function(o, p, s, precip, pr.threshold, theta) { \n      if (precip == FALSE) {\n            stop(\"method gpqm is only applied to precipitation data\")\n      } else {\n            threshold <- pr.threshold\n            if (any(!is.na(o))) {\n                  params <-  adjustPrecipFreq(o, p, threshold)\n                  p <- params$p\n                  nP <- params$nP\n                  Pth <- params$Pth\n            } else {\n                  nP = NULL\n            }\n            if (is.null(nP)) {\n                  s <- rep(NA, length(s))\n            } else if (nP[1] < length(o)) {\n                  ind <- which(o > threshold & !is.na(o))\n                  indgamma <- ind[which(o[ind] < quantile(o[ind], theta))]\n                  indpareto <- ind[which(o[ind] >= quantile(o[ind], theta))]\n                  obsGQM <- fitdistr(o[indgamma],\"gamma\")\n                  obsGQM2 <- fpot(o[indpareto], quantile(o[ind], theta), \"gpd\", std.err = FALSE)\n                  ind <- which(p > 0 & !is.na(p))\n                  indgammap <- ind[which(p[ind] < quantile(p[ind],theta))]\n                  indparetop <- ind[which(p[ind] >= quantile(p[ind], theta))]\n                  prdGQM <- fitdistr(p[indgammap], \"gamma\")\n                  prdGQM2 <- fpot(p[indparetop], quantile(p[ind], theta), \"gpd\", std.err = FALSE)\n                  rain <- which(s > Pth & !is.na(s))\n                  noRain <- which(s <= Pth & !is.na(s))\n                  indgammasim <- rain[which(s[rain] < quantile(p[ind], theta))]\n                  indparetosim <- rain[which(s[rain] >= quantile(p[ind], theta))]\n                  auxF <- pgamma(s[indgammasim], prdGQM$estimate[1], rate = prdGQM$estimate[2])\n                  auxF2 <- pgpd(s[indparetosim], loc = 0, scale = prdGQM2$estimate[1], shape = prdGQM2$estimate[2])\n                  s[indgammasim] <- qgamma(auxF, obsGQM$estimate[1], rate = obsGQM$estimate[2])\n                  s[indparetosim[which(auxF2 < 1)]] <- qgpd(auxF2[which(auxF2 < 1)], loc = 0, scale = obsGQM2$estimate[1], shape = obsGQM2$estimate[2])\n                  s[indparetosim[which(auxF2 == 1)]] <- max(o[indpareto], na.rm = TRUE)\n                  s[noRain] <- 0\n            } else {\n                  warning(\"There is at least one location without rainfall above the threshold.\\n In this (these) location(s) none bias correction has been applied.\")\n            }  \n      }\n      return(s)\n}\n\n#end\n\n\n#' @title Variance scaling of temperature\n#' @description Implementation of Variance scaling of temperature method for bias correction\n#' @param o A vector (e.g. station data) containing the observed climate data for the training period\n#' @param p A vector containing the simulated climate by the model for the training period. \n#' @param s A vector containing the simulated climate for the variable used in \\code{p}, but considering the test period.\n#' @param precip Logical indicating if o, p, s is temperature data.\n#' @keywords internal\n#' @author B. Szabo-Takacs\n\nvariance <- function(o, p, s, precip) {\n      if (precip == FALSE) {\n            t_dif <- mean(o, na.rm = TRUE) - mean(p, na.rm = TRUE)\n            t1 <- p + rep(t_dif, length(p), 1)\n            t1_m <- mean(t1,na.rm = TRUE) \n            t2 <- t1 - rep(t1_m,length(t1),1)\n            o_s <- sd(o,na.rm = TRUE) \n            t2_s <- sd(t2,na.rm = TRUE) \n            tsig <- o_s/t2_s\n            t1 <- t1_m <- t2 <- o_s <- t2_s <- NULL\n            t1 <- s + rep(t_dif, length(s), 1)\n            t1_m <- mean(t1, na.rm = TRUE)\n            t2 <- t1 - rep(t1_m, length(t1), 1)\n            t3 <- t2 * rep(tsig, length(t2), 1)\n            tC <- t3 + rep(t1_m, length(t3), 1)\n            t1 <- t1_m <- t2 <- t3 <- NULL\n            return(tC)\n      } else {\n            stop(\"method variance is only applied to temperature data\")\n      }\n}\n\n\n#' @title Local intensity scaling of precipitation\n#' @description Implementation of Local intensity scaling of precipitation method for bias correction based on Vincent Moron's local_scaling function in weaclim toolbox in Matlab\n#' @param o A vector (e.g. station data) containing the observed climate data for the training period\n#' @param p A vector containing the simulated climate by the model for the training or test period. \n#' @param s A vector containing the simulated climate for the variable used in \\code{p}, but considering the test period.\n#' @param precip Logical indicating if o, p, s is precipitation data.\n#' @param pr.threshold The minimum value that is considered as a non-zero precipitation. Ignored when \n#' \\code{precip = FALSE}. See details in function \\code{biasCorrection}.\n#' @author B. Szabo-Takacs\n\nloci <- function(o, p, s, precip, pr.threshold){\n      if (precip == FALSE) { \n            stop(\"method loci is only applied to precipitation data\")\n      } else {\n            threshold <- pr.threshold\n            l <- length(which(o > threshold))\n            gcmr <- rev(sort(p))\n            gcmrs <- rev(sort(s))\n            Pgcm <- gcmr[l + 1]\n            Pgcms <- gcmrs[l + 1]\n            mobs <- mean(o[which(o > threshold)], na.rm = TRUE)\n            mgcm <- mean(p[which(p > Pgcm)], na.rm = TRUE)\n            scaling <- (mobs - threshold) / (mgcm - Pgcm)\n            GCM <- (scaling*(s - Pgcms)) + threshold\n            GCM[which(GCM < threshold)] <- 0\n      }\n      return(GCM)\n}\n\n\n#' @title Power transformation of precipitation\n#' @description Implementation of Power transformation of precipitation method for bias correction \n#' @param o A vector (e.g. station data) containing the observed climate data for the training period\n#' @param p A vector containing the simulated climate by the model for the training period. \n#' @param s A vector containing the simulated climate for the variable used in \\code{p}, but considering the test period.\n#' @param precip Logical indicating if o, p, s is precipitation data.\n#' @importFrom stats uniroot\n#' @keywords internal\n#' @author S. Herrera and B. Szabo-Takacs\n\nptr <- function(o, p, s, precip) {\n      if (precip == FALSE) { \n            stop(\"method power transformation is only applied to precipitation data\")\n      } else {\n            b <- NaN\n            cvO <- sd(o,na.rm = TRUE) / mean(o, na.rm = TRUE)\n            if (!is.na(cvO)) {\n                  bi <- try(uniroot(function(x)\n                        varCoeficient(x, abs(p), cvO), c(0,1), extendInt = \"yes\"), silent = TRUE)\n                  if (\"try-error\" %in% class(bi)) {  # an error occurred\n                        b <- NA\n                  } else {\n                        b <- bi$root\n                  }\n            }\n            p[p < 0] <-  0\n            s[s < 0] <-  0\n            aux_c <- p^rep(b,length(p),1)\n            aux <- s^rep(b,length(s),1)\n            prC <- aux * rep((mean(o, na.rm = TRUE) / mean(aux_c, na.rm = TRUE)), length(s), 1)\n            aux <- aux_c <- NULL\n      }\n      return(prC)\n}\n\n\n#' @title VarCoeficient\n#' @description preprocess to power transformation of precipitation\n#' @param delta A vector of power parameter\n#' @param data A vector containing the simulated climate by the model for training period\n#' @param cv A vector containing coefficient of variation of observed climate data\n#' @keywords internal\n#' @author S. Herrera and B. Szabo-Takacs\n\nvarCoeficient <- function(delta,data,cv){\n      y <- cv - sd((data^delta), na.rm = TRUE)/mean((data^delta), na.rm = TRUE)\n      return(y)\n}\n\n\n#' @title Concatenate members\n#' @description Concatenate members as a single time series for using their joint distribution in bias correction\n#' @param grid Input (multimember) grid\n#' @return A grid without members, with additional attributes to retrieve the original structure after bias correction\n#' @seealso \\code{\\link{recoverMemberDim}}, for recovering the original structure after bias correction.\n#' @keywords internal\n#' @importFrom transformeR subsetGrid redim getShape bindGrid\n#' @author J Bedia\n\nflatMemberDim <- function(grid, station) {\n      grid <- redim(grid, member = TRUE, loc = station)     \n      n.mem.join <- getShape(grid, \"member\")\n      n.time.join <- getShape(grid, \"time\")\n      aux.ltime <- lapply(1:n.mem.join, function(x) {\n            subsetGrid(grid, members = x)\n      })\n      out <- do.call(\"bindGrid\", c(aux.ltime, dimension = \"time\"))\n      attr(out, \"orig.mem.shape\") <- n.mem.join\n      attr(out, \"orig.time.shape\") <- n.time.join\n      return(out)\n}\n\n#' @title Recover member multimember structure\n#' @description Recover member multimember structure after application of \\code{\\link{flatMemberDim}}\n#' @param plain.grid A \\dQuote{flattened} grid used as predictor in \\code{biasCorrection} (the 'pred' object)\n#' @param bc.grid The bias-corrected output (the 'bc' object), still without its member structure \n#' @param newdata The 'newdata' object, needed to recover relevant metadata (i.e. initialization dates and member names)\n#' @return A (bias-corrected) multimember grid\n#' @keywords internal\n#' @importFrom transformeR subsetDimension bindGrid\n#' @seealso \\code{\\link{flatMemberDim}}, for \\dQuote{flattening} the member structure\n#' @author J Bedia\n\nrecoverMemberDim <- function(plain.grid, bc.grid, newdata) {\n      bc <- bc.grid\n      nmem <- attr(plain.grid, \"orig.mem.shape\")\n      ntimes <- attr(plain.grid, \"orig.time.shape\")\n      # bc$Dates <- lapply(bc$Dates, \"rep\", nmem)\n      aux.list <- lapply(1:nmem, function(m) {\n            aux <- subsetDimension(grid = bc, dimension = \"time\", indices = seq(m, nmem * ntimes, nmem))\n            aux$InitializationDates <- newdata$InitializationDates[[m]]\n            aux$Members <- newdata$Members[[m]]\n            return(aux)\n      })\n      do.call(\"bindGrid\", c(aux.list, dimension = \"member\"))\n}\n\n\n#     biasCorrection.chunk.R Bias correction methods\n#\n#     Copyright (C) 2017 Santander Meteorology Group (http://www.meteo.unican.es)\n#\n#     This program is free software: you can redistribute it and/or modify\n#     it under the terms of the GNU General Public License as published by\n#     the Free Software Foundation, either version 3 of the License, or\n#     (at your option) any later version.\n# \n#     This program is distributed in the hope that it will be useful,\n#     but WITHOUT ANY WARRANTY; without even the implied warranty of\n#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#     GNU General Public License for more details.\n# \n#     You should have received a copy of the GNU General Public License\n#     along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n#' @title Bias correction methods applied to each latitude (chunk)\n#' @description Internal function that applies function \\code{biasCorrection} \n#' to each latitude in \\code{y} (chunk). \n#'\n#' @template templateObsPredSim\n#' @param output.path Path to the directory where the bias corrected *.rda objects are stored.\n#' Default is the working directory.\n#' @param method method applied. Current accepted values are \\code{\"eqm\"}, \\code{\"delta\"},\n#'  \\code{\"scaling\"}, \\code{\"pqm\"} and \\code{\"gpqm\"} \\code{\"variance\"},\\code{\"loci\"} and \\code{\"ptr\"}. See details.\n#' @param precipitation Logical for precipitation data (default to FALSE). If TRUE Adjusts precipitation \n#' frequency in 'x' (prediction) to the observed frequency in 'y'. This is a preprocess to bias correct \n#' precipitation data following ThemeÃŸl et al. (2012). To adjust the frequency, \n#' parameter \\code{wet.threshold} is used (see below).\n#' @param cross.val Logical (default to FALSE). Should cross-validation be performed? methods available are \n#' leave-one-out (\"loo\") and k-fold (\"kfold\") on an annual basis. The default option (\"none\") does not \n#' perform cross-validation.\n#' @param folds Only requiered if \\code{cross.val = \"kfold\"}. A list of vectors, each containing the years \n#' to be grouped in the corresponding fold.\n#' @param wet.threshold The minimum value that is considered as a non-zero precipitation. Ignored when \n#' \\code{precipitation = FALSE}. Default to 1 (assuming mm). See details on bias correction for precipitation.\n#' @param window vector of length = 2 (or 1) specifying the time window width used to calibrate and the \n#' target days (days that are being corrected). The window is centered on the target day/s \n#' (window width >= target days). Default to \\code{NULL}, which considers the whole period.\n#' @param scaling.type Character indicating the type of the scaling method. Options are \\code{\"additive\"} (default)\n#' or \\code{\"multiplicative\"} (see details). This argument is ignored if \\code{\"scaling\"} is not \n#' selected as the bias correction method.\n#' @param  fitdistr.args Further arguments passed to function \\code{\\link[MASS]{fitdistr}} \n#' (\\code{densfun}, \\code{start}, \\code{...}). Only used when applying the \"pqm\" method \n#' (parametric quantile mapping). Please, read the \\code{\\link[MASS]{fitdistr}} help \n#' document  carefully before setting the parameters in \\code{fitdistr.args}.\n#' @param n.quantiles Integer indicating the number of quantiles to be considered when method = \"eqm\". Default is NULL, \n#' that considers all quantiles, i.e. \\code{n.quantiles = length(x[i,j])} (being \\code{i} and \\code{j} the \n#' coordinates in a single location).\n#' @param extrapolation Character indicating the extrapolation method to be applied to correct values in  \n#' \\code{newdata} that are out of the range of \\code{x}. Extrapolation is applied only to the \\code{\"eqm\"} method, \n#' thus, this argument is ignored if other bias correction method is selected. Default is \\code{\"none\"} (do not extrapolate).\n#' @param theta numeric indicating  upper threshold (and lower for the left tail of the distributions, if needed) \n#' above which precipitation (temperature) values are fitted to a Generalized Pareto Distribution (GPD). \n#' Values below this threshold are fitted to a gamma (normal) distribution. By default, 'theta' is the 95th \n#' percentile (5th percentile for the left tail). Only for \\code{\"gpqm\"} method.\n#' @param join.members Logical indicating whether members should be corrected independently (\\code{FALSE}, the default),\n#'  or joined before performing the correction (\\code{TRUE}). It applies to multimember grids only (otherwise ignored).\n#' @template templateParallelParams\n#'  \n#' @details\n#' \n#' The methods available are \\code{\"eqm\"}, \\code{\"delta\"}, \n#' \\code{\"scaling\"}, \\code{\"pqm\"}, \\code{\"gpqm\"}\\code{\"loci\"}, \n#' \\code{\"ptr\"}  (the four latter used only for precipitation) and \n#' \\code{\"variance\"} (only for temperature).\n#' \n#'  These are next briefly described: \n#'  \n#' \\strong{Delta}\n#'\n#' This method consists on adding to the observations the mean change signal (delta method).\n#' This method is applicable to any kind of variable but it is preferable to avoid it for bounded variables\n#' (e.g. precipitation, wind speed, etc.) because values out of the variable range could be obtained\n#' (e.g. negative wind speeds...). This method corresponds to case g=1 and f=0 in Amengual et al. 2012. \n#' \n#' \\strong{Scaling}\n#' \n#' This method consists on scaling the simulation  with the difference (additive) or quotient (multiplicative) \n#' between the observed and simulated means in the train period. The \\code{additive} or \\code{multiplicative}\n#' correction is defined by parameter \\code{scaling.type} (default is \\code{additive}).\n#' The additive version is preferably applicable to unbounded variables (e.g. temperature) \n#' and the multiplicative to variables with a lower bound (e.g. precipitation, because it also preserves the frequency). \n#' \n#' \n#' \\strong{eqm}\n#' \n#' Empirical Quantile Mapping. This is a very extended bias correction method which consists on calibrating the simulated Cumulative Distribution Function (CDF) \n#' by adding to the observed quantiles both the mean delta change and the individual delta changes in the corresponding quantiles. \n#' This is equivalent to f=g=1 in Amengual et al. 2012. This method is applicable to any kind of variable.\n#' \n#' \n#' \\strong{pqm}\n#' \n#' Parametric Quantile Mapping. It is based on the initial assumption that both observed and simulated intensity distributions are well approximated by a given distribution\n#' (see \\code{\\link[MASS]{fitdistr}} to check available distributions), therefore is a parametric q-q map that uses the theorical instead of the empirical distribution.\n#' For instance, the gamma distribution is described in Piani et al. 2010 and is applicable to precipitation. Other example is the weibull distribution, which\n#' is applicable to correct wind data (Tie et al. 2014).\n#'  \n#' \\strong{gpqm}\n#'  \n#' Generalized Quantile Mapping (described in Gutjahr and Heinemann 2013) is also a parametric quantile mapping (see\n#' method 'pqm') but using two teorethical distributions, the gamma distribution and Generalized Pareto Distribution (GPD).\n#' By default, It applies a gamma distribution to values under the threshold given by the 95th percentile \n#' (following Yang et al. 2010) and a general Pareto distribution (GPD) to values above the threshold. the threshold above \n#' which the GPD is fitted is the 95th percentile of the observed and the predicted wet-day distribution, respectively. \n#' The user can specify a different threshold by modifying the parameter theta. It is applicable to precipitation data. \n#' \n#' \n#' \\strong{variance}\n#' \n#' Variance scaling of temperature. This method is described in Chen et al. 2011. It is applicable only to temperature. It corrects\n#' the mean and variance of temperature time series.\n#' \n#' \\strong{loci}\n#' \n#' Local intensity scaling of precipitation. This method is described in Schmidli et al. 2006. It adjust the mean as well as both wet-day frequencies and wet-day intensities.\n#' The precipitation threshold is calculated such that the number of simulated days exceeding this threshold matches the number of observed days with precipitation larger than 1 mm.\n#' \n#'\\strong{ptr}\n#'\n#' Power transformation of precipitation. This method is described in Leander and Buishand 2007 and is applicable only to precipitation. It adjusts the variance statistics of precipitation\n#' time series in an exponential form. The power parameter is estimated on a monthly basis using a 90-day window centered on the interval. The power is defined by matching the coefficient\n#' of variation of corrected daily simulated precipitation with the coefficient of variation of observed daily precipitation. It is calculated by root-finding algorithm using Brent's method.\n#'\n#'\n#' @section Note on the bias correction of precipitation:\n#' \n#' In the case of precipitation a frequency adaptation has been implemented in all versions of \n#' quantile mapping to alleviate the problems arising when the dry day frequency in the raw model output is larger\n#'  than in the observations (Wilcke et al. 2013). \n#'  \n#'  The precipitation subroutines are switched-on when the variable name of the grid \n#'  (i.e., the value returned by \\code{gridData$Variable$varName}) is one of the following: \n#'  \\code{\"pr\"}, \\code{\"tp\"} (this is the standard name defined in the vocabulary (\\code{\\link[loadeR]{C4R.vocabulary}}), \\code{\"precipitation\"} or \\code{\"precip\"}.\n#'  Thus, caution must be taken to ensure that the correct bias correction is being undertaken when dealing with\n#'  non-standard variables.\n#'     \n#' \n#' @seealso \\code{\\link{isimip}} for a trend-preserving method of model calibration and \\code{\\link{quickDiagnostics}} \n#' for an outlook of the results.\n#' @return Calibrated grids for each latitudinal chunk (with the same spatio-temporal extent than the chunked input \\code{\"y\"}). \n#' This objects are saved in the specified \\code{output.path}. The object obatained in the workspace \n#' is a charecter string of the listed files.\n#' @family downscaling\n#' \n#' @importFrom transformeR redim subsetGrid getYearsAsINDEX getDim\n#' @importFrom abind adrop\n#'\n#' @references\n#'\n#' \\itemize{\n#' \\item R.A.I. Wilcke, T. Mendlik and A. Gobiet (2013) Multi-variable error correction of regional climate models. Climatic Change, 120, 871-887\n#'\n#' \\item A. Amengual, V. Homar, R. Romero, S. Alonso, and C. Ramis (2012) A Statistical Adjustment of Regional Climate Model Outputs to Local Scales: Application to Platja de Palma, Spain. J. Clim., 25, 939-957\n#'\n#' \\item C. Piani, J. O. Haerter and E. Coppola (2009) Statistical bias correction for daily precipitation in regional climate models over Europe, Theoretical and Applied Climatology, 99, 187-192\n#'\n#' \\item O. Gutjahr and G. Heinemann (2013) Comparing precipitation bias correction methods for high-resolution regional climate simulations using COSMO-CLM, Theoretical and Applied Climatology, 114, 511-529\n#' \n#' \\item M. R. Tye, D. B. Stephenson, G. J. Holland and R. W. Katz (2014) A Weibull Approach for Improving Climate Model Projections of Tropical Cyclone Wind-Speed Distributions, Journal of Climate, 27, 6119-6133\n#' \n#' }\n#' @author M. Iturbide\n#' @examples {\n#' # empirical\n#' eqm1 <- biasCorrection.chunk(output.path = getwd(),\n#'                              n.chunks = 10,\n#'                              login.UDG,\n#'                              dataset.y = \"WATCH_WFDEI\",\n#'                              dataset.x = \"CORDEX-EUR11_EC-EARTH_r12i1p1_historical_RCA4_v1\",\n#'                              dataset.newdata = \"CORDEX-EUR11_EC-EARTH_r12i1p1_historical_RCA4_v1\",\n#'                              loadGridData.args = list(var = \"tasmax\", \n#'                                                       years = 1971:2000, \n#'                                                       lonLim = c(-10, 10), \n#'                                                       latLim = c(36, 45)),\n#'                              biasCorrection.args = list(precipitation = FALSE,\n#'                                                         method = c(\"delta\", \"scaling\", \"eqm\", \"pqm\", \"gpqm\", \"loci\"),\n#'                                                         cross.val = c(\"none\", \"loo\", \"kfold\"),\n#'                                                         folds = NULL,\n#'                                                         window = NULL,\n#'                                                         scaling.type = c(\"additive\", \"multiplicative\"),\n#'                                                         fitdistr.args = list(densfun = \"normal\"),\n#'                                                         wet.threshold = 1,\n#'                                                         n.quantiles = NULL,\n#'                                                         extrapolation = c(\"none\", \"constant\"), \n#'                                                         theta = .95,\n#'                                                         join.members = FALSE,\n#'                                                         parallel = FALSE,\n#'                                                         max.ncores = 16,\n#'                                                         ncores = NULL))\n#' }\n\n# eqm1 <- biasCorrection.chunk(output.path = getwd(),\n#                              n.chunks = 10,\n#                              loginUDG.args = list(username = \"miturbide\", password = \"iturbide.14\"),\n#                              dataset.y = \"WATCH_WFDEI\",\n#                              dataset.x = \"CORDEX-EUR11_EC-EARTH_r12i1p1_historical_RCA4_v1\",\n#                              dataset.newdata = \"CORDEX-EUR11_EC-EARTH_r12i1p1_historical_RCA4_v1\",\n#                              loadGridData.args = list(var = \"tasmax\",\n#                                                       years = 1971:2000,\n#                                                       lonLim = c(-10, 10),\n#                                                       latLim = c(36, 45)),\n#                              biasCorrection.args = list(precipitation = FALSE,\n#                                                         method = c(\"delta\", \"scaling\", \"eqm\", \"pqm\", \"gpqm\", \"loci\"),\n#                                                         cross.val = c(\"none\", \"loo\", \"kfold\"),\n#                                                         folds = NULL,\n#                                                         window = NULL,\n#                                                         scaling.type = c(\"additive\", \"multiplicative\"),\n#                                                         fitdistr.args = list(densfun = \"normal\"),\n#                                                         wet.threshold = 1,\n#                                                         n.quantiles = NULL,\n#                                                         extrapolation = c(\"none\", \"constant\"),\n#                                                         theta = .95,\n#                                                         join.members = FALSE,\n#                                                         parallel = FALSE,\n#                                                         max.ncores = 16,\n#                                                         ncores = NULL))\n# \n# biasCorrection.chunk <- function(output.path = getwd(),\n#                            n.chunks = 10,\n#                            loginUDG.args = list(NULL),\n#                            dataset.y = \"WATCH_WFDEI\",\n#                            dataset.x = \"CORDEX-EUR11_EC-EARTH_r12i1p1_historical_RCA4_v1\",\n#                            dataset.newdata = \"CORDEX-EUR11_EC-EARTH_r12i1p1_historical_RCA4_v1\",\n#                            loadGridData.args = list(var = \"tasmax\", \n#                                                     years = 1971:2000, \n#                                                     lonLim = c(-10, 10), \n#                                                     latLim = c(36, 45)),\n#                            biasCorrection.args = list(precipitation = FALSE,\n#                               method = c(\"delta\", \"scaling\", \"eqm\", \"pqm\", \"gpqm\", \"loci\"),\n#                               cross.val = c(\"none\", \"loo\", \"kfold\"),\n#                               folds = NULL,\n#                               window = NULL,\n#                               scaling.type = c(\"additive\", \"multiplicative\"),\n#                               fitdistr.args = list(densfun = \"normal\"),\n#                               wet.threshold = 1,\n#                               n.quantiles = NULL,\n#                               extrapolation = c(\"none\", \"constant\"), \n#                               theta = .95,\n#                               join.members = FALSE,\n#                               parallel = FALSE,\n#                               max.ncores = 16,\n#                               ncores = NULL)) {\n#       suppressWarnings(dir.create(output.path))\n#       message(\"[\", Sys.time(), \"] Rdata will be saved in \", output.path)\n#       do.call(\"loginUDG\", login.UDG)\n#       di.y <- dataInventory(dataset.y)\n#       lats.y <- di.y[[loadGridData.args[[\"var\"]]]]$Dimensions$lat$Values\n#       if (is.null(lats.y)) stop(\"dataset.y does not contain the requested variable\")\n#       lats.y <- lats.y[which.min(abs(lats.y - latLim[1]))[1]:(which.min(abs(lats.y - latLim[2]))[1] + 1)]\n#       n.lats.y <- length(lats.y)\n#       n.lat.chunk <- ceiling(n.lats.y/n.chunks)\n#       aux.ind <- rep(1:(n.chunks - 1), each = n.lat.chunk)\n#       ind <- c(aux.ind, rep((max(aux.ind) + 1), each = n.lats.y - length(aux.ind)))\n#       message(\"[\", Sys.time(), \"] y contains \", n.lats.y, \" latitudes. Bias correction will be applied in \",n.chunks, \" chunks of about \", n.lat.chunk, \" latitudes.\")\n#       lat.list <- split(lats.y, f = ind)\n#       lat.range.chunk <- lapply(lat.list, range)\n#       lat.range.chunk.x <- lapply(lat.range.chunk, function(x) c(x[1] - 3, x[2] + 3))\n#       \n#       file.dir <- character()\n#       for (i in 1:length(lat.range.chunk)) {\n#             loadGridData.args[[\"dataset\"]] <- dataset.y\n#             loadGridData.args[[\"latLim\"]] <- lat.range.chunk[[i]]\n#             y <- do.call(\"loadGridData\", loadGridData.args)\n#             loadGridData.args[[\"dataset\"]] <- dataset.x\n#             loadGridData.args[[\"latLim\"]] <- lat.range.chunk.x[[i]]\n#             x <- do.call(\"loadGridData\", loadGridData.args)\n#             if (!is.null(dataset.newdata)) {\n#                   loadGridData.args[[\"dataset\"]] <- dataset.newdata\n#                   newdata <- do.call(\"loadGridData\", loadGridData.args)\n#             } else {\n#                   newdata <- NULL\n#             }\n#             bc <- do.call(\"biasCorrection\", biasCorrection.args)\n#             file.dir[i] <- paste0(output.path, \"/chunk\", i, \".rda\")\n#             save(bc, file = paste0(output.path, \"/chunk\", i, \".rda\"))\n#       }\n#       return(file.dir)\n# }\n# \n",
    "created" : 1551194534189.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1785905905",
    "id" : "35497CEF",
    "lastKnownWriteTime" : 1549892263,
    "last_content_update" : 1549892263,
    "path" : "/media/maialen/work/WORK/GIT/downscaleR/R/biasCorrection.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}